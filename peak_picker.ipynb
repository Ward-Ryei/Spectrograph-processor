{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bd62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"PC_ms1.db\",'rb') as f:\n",
    "    PC_db=pickle.load(f)\n",
    "    PC_db['PRECURSORMZ']=PC_db['PRECURSORMZ'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0505f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymzml\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "def get_PC_ms1(file,delta=10e-6):\n",
    "    TIC=pymzml.run.Reader(file)\n",
    "    pc_num=0\n",
    "    total_num=TIC.get_spectrum_count()\n",
    "    spect_df=pd.DataFrame(columns=['m/z','inten','sid','low_win','high_win'])\n",
    "    \n",
    "    '''###\n",
    "    spec1=TIC[1]\n",
    "    pority=spec1['MS:1000129']\n",
    "    print(pority,type(pority))\n",
    "    '''###\n",
    "    \n",
    "    #return \n",
    "\n",
    "    for spectrum in TIC:\n",
    "        #print(spectrum.ID,type(spectrum.ID))\n",
    "        #print(f\"handling the {spectrum.ID}th /{total_num} spectrum\")\n",
    "        if spectrum['MS:1000129'] or spectrum['MS:1000511']==1:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        if spectrum.has_peak(184.07294):\n",
    "            current_mz=spectrum['MS:1000744']\n",
    "            current_inten=spectrum['MS:1000042']\n",
    "            current_sid=spectrum.ID\n",
    "            current_low_win  =current_mz-spectrum['MS:1000828']\n",
    "            current_hight_win=current_mz+spectrum['MS:1000829']\n",
    "            output=[current_mz,current_inten,current_sid,current_low_win,current_hight_win]\n",
    "            #current_pority=1 if spec1['MS:1000130'] else 2\n",
    "            if spect_df.empty:\n",
    "                spect_df.loc[len(spect_df),:]=output\n",
    "                pc_num+=1\n",
    "                continue\n",
    "            if  abs( (spect_df['m/z']-current_mz)/current_mz).min()>delta:\n",
    "                spect_df.loc[len(spect_df),:]=output\n",
    "                pc_num+=1\n",
    "            else:#update\n",
    "                hit=(abs(spect_df['m/z']-current_mz) == abs(spect_df['m/z']-current_mz).min())\n",
    "                \n",
    "                #print(hit,f'\\n the hit num is {hit.sum()}\\n',spect_df.loc[hit,:])\n",
    "                if (spect_df.loc[hit,'inten']<current_inten).all():\n",
    "                    spect_df.loc[hit,:]=output\n",
    "    return spect_df\n",
    "\n",
    "def get_standerd_ms1(file,delta=5e-6,measured_precision=5e-6):\n",
    "    TIC=pymzml.run.Reader(file)\n",
    "    pc_num=0\n",
    "    total_num=TIC.get_spectrum_count()\n",
    "    #TIC[0]\n",
    "    #return \n",
    "    ms1_df=pd.DataFrame(columns=['sid','inten'])\n",
    "    for spectrum in TIC:\n",
    "        if spectrum['MS:1000130'] and spectrum['MS:1000511']==1:\n",
    "            ms1_df.loc[len(ms1_df),:]=[spectrum.ID,spectrum['MS:1000285']]\n",
    "    #print(ms1_df)\n",
    "    \n",
    "    inten=ms1_df['inten'].values\n",
    "    hit= (ms1_df['inten']> np.percentile(inten,80)) & (ms1_df['inten']<= np.percentile(inten,90))\n",
    "    wanted=ms1_df.loc[hit,:]\n",
    "    spectrum=pymzml.spec.Spectrum()\n",
    "    for item in wanted['sid']:\n",
    "        spectrum=spectrum+TIC[item]\n",
    "    spectrum=spectrum/len(wanted)\n",
    "    spectrum.peaks('reprofiled')\n",
    "    spectrum.measured_precision=measured_precision\n",
    "    spectrum.base_peak_inten=np.array(spectrum.peaks('reprofiled')).max(axis=0)[1]#centroided\n",
    "    #print(spectrum.peaks('reprofiled'))\n",
    "    return spectrum\n",
    "def indentify_mz_with_db(df,delta=5e-6):\n",
    "    def deal(mz):\n",
    "        #print(mz,type(mz))\n",
    "        err_count=abs(PC_db['PRECURSORMZ']-mz)/mz\n",
    "        hit= err_count <= delta\n",
    "        if hit.sum()>0:\n",
    "            name=PC_db.loc[err_count==err_count.min(),:].iloc[0,:][\"NAME\"]\n",
    "            return name\n",
    "        else:\n",
    "            return None\n",
    "    temp_df_in_indentify=copy.deepcopy(df)\n",
    "    temp_df_in_indentify['NAME']=temp_df_in_indentify['m/z'].apply(deal)\n",
    "    \n",
    "    #temp_df_in_indentify.drop_duplicates(subset=[])\n",
    "    #去重，保留inten大的重复值\n",
    "    identified_df_output=pd.DataFrame(columns=temp_df_in_indentify.columns)\n",
    "    for idx,ser in temp_df_in_indentify.iterrows():\n",
    "        hit=identified_df_output['NAME']==ser['NAME']\n",
    "        if hit.sum()>0:\n",
    "            hit_item=identified_df_output.iloc[hit.argmax(),:]\n",
    "            if hit_item['inten']<ser['inten']:\n",
    "                identified_df_output.iloc[hit.argmax(),:]=ser\n",
    "        \n",
    "        else:\n",
    "            identified_df_output.loc[len(identified_df_output),:]=ser\n",
    "    \n",
    "    \n",
    "    return identified_df_output\n",
    "def indentify_mz_with_db2(df,delta=10e-6):\n",
    "    def deal(ser):\n",
    "        mz=ser['m/z']\n",
    "        #print(mz,type(mz))\n",
    "        #outlet=[name,error,ion_type]\n",
    "        outlet=[None,None,None]\n",
    "        err_count=abs(PC_db['PRECURSORMZ']-mz)/mz\n",
    "        hit= err_count <= delta\n",
    "        if hit.sum()>0:\n",
    "            min_error=err_count.min()\n",
    "            got_item=PC_db.loc[err_count==min_error,:].iloc[0,:]\n",
    "            name=got_item[\"NAME\"]\n",
    "            outlet[0]=name\n",
    "            outlet[1]=min_error*1e6\n",
    "            outlet[2]=got_item[\"PRECURSORTYPE\"]\n",
    "            return outlet\n",
    "        else:\n",
    "            return outlet\n",
    "    temp_df_in_indentify=copy.deepcopy(df)\n",
    "    temp_df_in_indentify[['NAME','error','type']]=temp_df_in_indentify.apply(deal,result_type='expand',axis=1)\n",
    "    return temp_df_in_indentify\n",
    "def from_standerd_spectrum_get_inten(standerd_spectrum,is_absolute_inten=True):\n",
    "    local_standerd_spectrum=standerd_spectrum\n",
    "    #print(\"        current spectrum's base inten is:\",local_standerd_spectrum.base_peak_inten)\n",
    "    def deal(ser):\n",
    "        #print(ser)\n",
    "        mz=ser['m/z']\n",
    "        inten=ser['inten']\n",
    "        hit=local_standerd_spectrum.has_peak(mz)\n",
    "        \n",
    "        if len(hit)>0:\n",
    "            if is_absolute_inten:\n",
    "                return np.array(hit).max(axis=0)[1]\n",
    "            else:\n",
    "                return np.array(hit).max(axis=0)[1]/local_standerd_spectrum.base_peak_inten\n",
    "        else:\n",
    "            print(mz,' not found')\n",
    "            return 0\n",
    "    return deal\n",
    "def merge_groups_meth2(*dfs):\n",
    "    #单纯的鉴定结果联合\n",
    "    output=pd.DataFrame(columns=dfs[0].columns)\n",
    "    for temp_df in dfs:\n",
    "        for idx,item in temp_df.iterrows():\n",
    "            name_bits=output['NAME']==item['NAME']\n",
    "            type_bits=output['type']==item['type']\n",
    "            if name_bits.sum()>0:\n",
    "                \n",
    "                #完全一样,找偏差小的\n",
    "                got=(name_bits & type_bits)\n",
    "                if got.sum()>0:\n",
    "                    if output.iloc[got.argmax(),:]['error']>item['error']:\n",
    "                        output.iloc[got.argmax(),:]=item\n",
    "                \n",
    "                else:#类型不一样，优先保留加H\n",
    "                    if item['type']=='[M+H]+':\n",
    "                        output.loc[name_bits.argmax(),:]=item\n",
    "            \n",
    "            else:#m没有重复，直接添加\n",
    "                #print('success add')\n",
    "                output.loc[len(output),:]=item\n",
    "            \n",
    "    return output\n",
    "def merge_groups(*dfs):\n",
    "    #result=pd.DataFrame()\n",
    "    dfs=copy.deepcopy(dfs)\n",
    "    try:\n",
    "        [x.set_index('NAME',inplace=True) for x in dfs]\n",
    "        for col in ['index','inten','m/z']:\n",
    "            try:\n",
    "                [x.drop([col],axis=1, inplace=True) for x in dfs]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    res_merge= reduce(lambda left,right: pd.merge(left,right,left_index=True, right_index=True), dfs)\n",
    "    return  res_merge\n",
    "def filter_columns(df_input,therhold=0.3):\n",
    "    pass\n",
    "    inten_columns=[x for x in df_input.columns if \"mzML\" in x]\n",
    "    hit=(df_input.loc[:,inten_columns]>0).sum(axis=1) > therhold*len(inten_columns)\n",
    "    #print(hit)\n",
    "    output_df=copy.deepcopy(df_input.loc[hit,:] )\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "701d3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "def get_all_group_pc(para,pricession=10e-5):\n",
    "    #### PC 确定的时候，有可能是因为没有采集到二级而鉴定失败，所以再排除不是二级的质谱图时，以全部组别混合的二级PC进行排除 stage I 获得全组的PC二级\n",
    "    pricession=10e-5\n",
    "    all_group_pc=pd.DataFrame()\n",
    "    for idx,content in para.iterrows():\n",
    "        #if content['group']!=group_id:continue\n",
    "        mzml_file=os.path.join(content['dir'],content['files'])\n",
    "        \n",
    "        current_file_pc_df=get_PC_ms1(mzml_file)\n",
    "        current_file_pc_df['group']=content['group']\n",
    "        print(f\"the current mzml_file is{os.path.split(mzml_file)[1]},the len of  current_file_pc_df is {len(current_file_pc_df)}\")\n",
    "        if all_group_pc.empty:\n",
    "            print(\"        it is the frist mzml\")\n",
    "            all_group_pc=current_file_pc_df\n",
    "        else:\n",
    "            for j_idx,j_content in current_file_pc_df.iterrows():\n",
    "                hit=abs(all_group_pc['m/z']-j_content['m/z'])/j_content['m/z']<=pricession\n",
    "                if hit.sum()==0:\n",
    "                    all_group_pc=all_group_pc.append(j_content)\n",
    "\n",
    "        print(f\"        contained the {idx} file , the len of group_pc is {len(all_group_pc)}\" )\n",
    "    print(all_group_pc.shape)\n",
    "    return all_group_pc\n",
    "def all_groups_pc_for_get_group_result(group_id,all_group_pc,para,measured_precision=10e-5):\n",
    "    #stage I\n",
    "    group_id=group_id\n",
    "    measured_precision=measured_precision\n",
    "    group_peak=pd.DataFrame()\n",
    "    for idx,content in para.iterrows():\n",
    "        if content['group']!=group_id:continue\n",
    "        mzml_file=os.path.join(content['dir'],content['files'])\n",
    "        standerd_ms1=get_standerd_ms1(mzml_file)\n",
    "        standerd_ms1.measured_precision=measured_precision\n",
    "        current_peaks_df=pd.DataFrame(standerd_ms1.peaks('centroided'),columns=['m/z','inten'])\n",
    "\n",
    "        if group_peak.empty:\n",
    "            group_peak=current_peaks_df\n",
    "        else:\n",
    "            for j_idx,j_content in current_peaks_df.iterrows():\n",
    "                #print(group_peak)\n",
    "                #print(j_content)\n",
    "                hit=abs(group_peak['m/z']-j_content['m/z'])/j_content['m/z']<=measured_precision\n",
    "                if hit.sum()==0:\n",
    "                    group_peak=group_peak.append(j_content)\n",
    "    print(group_peak.shape)\n",
    "    group_peak\n",
    "\n",
    "\n",
    "\n",
    "    ##stage III\n",
    "    group_peak_pc_df=pd.DataFrame(columns=group_peak.columns)\n",
    "    for idx,content in group_peak.iterrows():\n",
    "        #print(group_peak_pc_df)\n",
    "        #print(content)\n",
    "        hit= (all_group_pc['low_win']<= content['m/z']) & (all_group_pc['high_win']>= content['m/z'])  \n",
    "        if hit.sum()>0:\n",
    "            group_peak_pc_df.loc[len(group_peak_pc_df),:]=content\n",
    "    print(group_peak_pc_df.shape)\n",
    "    group_peak_pc_df\n",
    "\n",
    "    ##stage IV \n",
    "    identified_df_group_m2_group1=indentify_mz_with_db(group_peak_pc_df,measured_precision).dropna()\n",
    "    print(identified_df_group_m2_group1.shape)\n",
    "    identified_df_group_m2_group1\n",
    "\n",
    "    ##stage V\n",
    "    for idx,content in para.iterrows():\n",
    "        if content['group']!=group_id:continue\n",
    "        mzml_file=os.path.join(content['dir'],content['files'])\n",
    "        standerd_ms_temp=get_standerd_ms1(mzml_file)\n",
    "        identified_df_group_m2_group1[content['files']]=identified_df_group_m2_group1.apply(from_standerd_spectrum_get_inten(standerd_ms_temp,False),axis=1)\n",
    "    identified_df_group_m2_group1\n",
    "    \n",
    "    \n",
    "    # 不要碳链长度\n",
    "    return identified_df_group_m2_group1\n",
    "def all_groups_pc_for_get_all_PC_indentified_result(group_id,all_group_pc,para,measured_precision=5e-5):\n",
    "    group_id=group_id\n",
    "    measured_precision=measured_precision\n",
    "    group_peak=pd.DataFrame()\n",
    "    for idx,content in para.iterrows():\n",
    "        if content['group']!=group_id:continue\n",
    "        mzml_file=os.path.join(content['dir'],content['files'])\n",
    "        standerd_ms1=get_standerd_ms1(mzml_file)\n",
    "        standerd_ms1.measured_precision=measured_precision\n",
    "        current_peaks_df=pd.DataFrame(standerd_ms1.peaks('centroided'),columns=['m/z','inten'])\n",
    "\n",
    "        if group_peak.empty:\n",
    "            group_peak=current_peaks_df\n",
    "        else:\n",
    "            for j_idx,j_content in current_peaks_df.iterrows():\n",
    "                #print(group_peak)\n",
    "                #print(j_content)\n",
    "                hit=abs(group_peak['m/z']-j_content['m/z'])/j_content['m/z']<=measured_precision\n",
    "                if hit.sum()==0:\n",
    "                    group_peak=group_peak.append(j_content)\n",
    "    print(group_peak.shape)\n",
    "    group_peak\n",
    "\n",
    "\n",
    "\n",
    "    ##stage III\n",
    "    group_peak_pc_df=pd.DataFrame(columns=group_peak.columns)\n",
    "    for idx,content in group_peak.iterrows():\n",
    "        #print(group_peak_pc_df)\n",
    "        #print(content)\n",
    "        hit= (all_group_pc['low_win']<= content['m/z']) & (all_group_pc['high_win']>= content['m/z'])  \n",
    "        if hit.sum()>0:\n",
    "            group_peak_pc_df.loc[len(group_peak_pc_df),:]=content\n",
    "    print(group_peak_pc_df.shape)\n",
    "    group_peak_pc_df\n",
    "\n",
    "    ##stage IV \n",
    "    identified_df_group_m2_group1=indentify_mz_with_db2(group_peak_pc_df,measured_precision).dropna()\n",
    "    print(identified_df_group_m2_group1.shape)\n",
    "    identified_df_group_m2_group1\n",
    "\n",
    "    \n",
    "    \n",
    "    # 不要碳链长度\n",
    "    return identified_df_group_m2_group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27c5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current mzml_file is20221014_iPicoESI_APCI_A549_P&N_lipid_01.mzML,the len of  current_file_pc_df is 141\n",
      "        it is the frist mzml\n",
      "        contained the 0 file , the len of group_pc is 141\n",
      "the current mzml_file is20221014_iPicoESI_APCI_HCCLM_P&N_lipid_24.mzML,the len of  current_file_pc_df is 122\n",
      "        contained the 1 file , the len of group_pc is 199\n",
      "(199, 6)\n",
      "(1487, 2)\n",
      "(375, 2)\n",
      "(192, 3)\n",
      "(1127, 2)\n",
      "(322, 2)\n",
      "(181, 3)\n",
      "          20221014_iPicoESI_APCI_A549_P&N_lipid_01.mzML  \\\n",
      "NAME                                                      \n",
      "PC 17:0                                        0.052482   \n",
      "SM 20:3                                        0.000303   \n",
      "PC 27:0                                        0.001577   \n",
      "PC 26:3                                        0.008134   \n",
      "SM 31:5                                        0.002457   \n",
      "...                                                 ...   \n",
      "SM 68:13                                       0.013299   \n",
      "PC 66:18                                       0.024908   \n",
      "PC 66:17                                       0.045400   \n",
      "PC 66:16                                       0.070370   \n",
      "PC 68:19                                       0.018288   \n",
      "\n",
      "          20221014_iPicoESI_APCI_HCCLM_P&N_lipid_24.mzML  \n",
      "NAME                                                      \n",
      "PC 17:0                                         0.005620  \n",
      "SM 20:3                                         0.001982  \n",
      "PC 27:0                                         0.099998  \n",
      "PC 26:3                                         0.004428  \n",
      "SM 31:5                                         0.000563  \n",
      "...                                                  ...  \n",
      "SM 68:13                                        0.001459  \n",
      "PC 66:18                                        0.013407  \n",
      "PC 66:17                                        0.003676  \n",
      "PC 66:16                                        0.033760  \n",
      "PC 68:19                                        0.009386  \n",
      "\n",
      "[157 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def demo_deal(raw_file_path):\n",
    "    import pandas as pd\n",
    "    para=pd.read_excel(raw_file_path)\n",
    "    all_group_pc=get_all_group_pc(para,pricession=10e-5)\n",
    "    all_group_dfs=[]\n",
    "    for group_id in para['group'].unique():\n",
    "        all_group_dfs.append(all_groups_pc_for_get_group_result(group_id,all_group_pc,para,measured_precision=10e-5))\n",
    "\n",
    "    result_merge=merge_groups(*all_group_dfs).fillna(0)\n",
    "    \n",
    "    \n",
    "    filter_df=filter_columns(result_merge,0.4)\n",
    "    filter_df.to_excel('for_pic.xlsx')\n",
    "demo_deal(r'./mzML_files.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe54a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
